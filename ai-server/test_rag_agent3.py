"""
RAG Agent 3 ì‹¤í—˜/í…ŒìŠ¤íŠ¸ ì½”ë“œ
- ë¡œì»¬ íŒŒì¼ ì¶”ê°€ í…ŒìŠ¤íŠ¸
- URL ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸
- ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
- ì§ˆë¬¸ë‹µë³€ í…ŒìŠ¤íŠ¸
"""
import asyncio
import os
from pathlib import Path
from agents.rag_agent3 import RAGAgent

async def test_rag_agent3():
    """RAG Agent 3 ì¢…í•© ì‹¤í—˜"""
    print("ğŸš€ RAG Agent 3 ì‹¤í—˜ ì‹œì‘...")
    
    # OpenAI API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("í™˜ê²½ë³€ìˆ˜ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    try:
        # RAG Agent 3 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        print("\nğŸ“¦ RAG Agent 3 ì´ˆê¸°í™”...")
        rag_agent = RAGAgent()
        
        # 1. ê¸°ì¡´ ë¡œì»¬ ë¬¸ì„œ í™•ì¸
        print("\nğŸ“ ê¸°ì¡´ ë¡œì»¬ ë¬¸ì„œ í™•ì¸...")
        docs_path = Path("data/docs")
        if docs_path.exists():
            txt_files = list(docs_path.glob("*.txt"))
            pdf_files = list(docs_path.glob("*.pdf"))
            docx_files = list(docs_path.glob("*.docx"))
            
            print(f"  - TXT íŒŒì¼: {len(txt_files)}ê°œ")
            for f in txt_files[:3]:  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                print(f"    â€¢ {f.name}")
            print(f"  - PDF íŒŒì¼: {len(pdf_files)}ê°œ")
            for f in pdf_files[:3]:
                print(f"    â€¢ {f.name}")
            print(f"  - DOCX íŒŒì¼: {len(docx_files)}ê°œ")
            for f in docx_files[:3]:
                print(f"    â€¢ {f.name}")
        
        # 2. í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¶”ê°€ (ë¡œì»¬ íŒŒì¼ + URL)
        print("\nğŸ“ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¶”ê°€...")
        
        # ë¡œì»¬ íŒŒì¼ ê²½ë¡œë“¤ (ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ë“¤)
        local_sources = []
        if docs_path.exists():
            local_sources.extend([str(f) for f in docs_path.glob("*.txt")][:2])
            local_sources.extend([str(f) for f in docs_path.glob("*.pdf")][:1])
        
        # í…ŒìŠ¤íŠ¸ìš© URLë“¤ (ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ URL)
        test_urls = [
            # GitHub README ì˜ˆì‹œ (Raw URL)
            "https://raw.githubusercontent.com/microsoft/vscode/main/README.md",
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ íŒŒì¼ ì˜ˆì‹œ
            "https://www.w3.org/TR/PNG/iso_8859-1.txt",
        ]
        
        # í…ŒìŠ¤íŠ¸í•  ì†ŒìŠ¤ë“¤ ê²°í•©
        test_sources = local_sources + test_urls
        
        if test_sources:
            print(f"ì¶”ê°€í•  ë¬¸ì„œ ì†ŒìŠ¤: {len(test_sources)}ê°œ")
            for i, src in enumerate(test_sources):
                if src.startswith("http"):
                    print(f"  {i+1}. [URL] {src}")
                else:
                    print(f"  {i+1}. [ë¡œì»¬] {Path(src).name}")
            
            # ë¬¸ì„œ ì¶”ê°€ ì‹¤í–‰
            print("\nâ³ ë¬¸ì„œ ì¶”ê°€ ì¤‘...")
            result = await rag_agent.add_documents(test_sources)
            print(f"âœ… ê²°ê³¼: {result}")
        else:
            print("âš ï¸  í…ŒìŠ¤íŠ¸í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ ë¬¸ì„œë¡œ ì§„í–‰...")
        
        # 3. ë²¡í„° ì €ì¥ì†Œ ìƒíƒœ í™•ì¸
        print("\nğŸ” ë²¡í„° ì €ì¥ì†Œ ìƒíƒœ í™•ì¸...")
        if rag_agent.vectorstore:
            print("âœ… ë²¡í„° ì €ì¥ì†Œê°€ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # FAISS íŒŒì¼ í™•ì¸
            vector_db_path = Path("data/vector_db")
            if (vector_db_path / "index.faiss").exists():
                file_size = (vector_db_path / "index.faiss").stat().st_size
                print(f"ğŸ’¾ FAISS ì¸ë±ìŠ¤ íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
        else:
            print("âŒ ë²¡í„° ì €ì¥ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 4. ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\nğŸ” ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
        test_queries = [
            "í”„ë¡œì íŠ¸ì— ëŒ€í•´ ì•Œë ¤ì¤˜",
            "API ë¬¸ì„œëŠ” ì–´ë””ì— ìˆë‚˜ìš”?",
            "ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ëª…í•´ì¤˜",
            "ì„¤ì¹˜ ë°©ë²•ì„ ì•Œë ¤ì¤˜"
        ]
        
        retriever = rag_agent.vectorstore.as_retriever(search_kwargs={"k": 3})
        
        for query in test_queries:
            print(f"\nğŸ“‹ ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'")
            try:
                docs = retriever.get_relevant_documents(query)
                print(f"  ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ ë¬¸ì„œ")
                
                for i, doc in enumerate(docs):
                    preview = doc.page_content[:100].replace('\n', ' ')
                    source = doc.metadata.get('source', 'Unknown')
                    print(f"    {i+1}. {preview}... [ì¶œì²˜: {Path(source).name}]")
                    
            except Exception as e:
                print(f"  âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # 5. ì§ˆë¬¸ë‹µë³€ í…ŒìŠ¤íŠ¸
        print("\nğŸ’¬ ì§ˆë¬¸ë‹µë³€ í…ŒìŠ¤íŠ¸...")
        qa_queries = [
            "í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ì„¤ì¹˜í•˜ëŠ” ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì•Œë ¤ì£¼ì„¸ìš”",
            "ì´ ì‹œìŠ¤í…œì˜ ì•„í‚¤í…ì²˜ëŠ” ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆë‚˜ìš”?"
        ]
        
        for query in qa_queries:
            print(f"\nâ“ ì§ˆë¬¸: {query}")
            try:
                answer = await rag_agent.process(query)
                print(f"ğŸ’¡ ë‹µë³€: {answer}")
                print("-" * 50)
            except Exception as e:
                print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
        
        # 6. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        print("\nâš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
        import time
        
        perf_query = "í”„ë¡œì íŠ¸ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”"
        start_time = time.time()
        
        try:
            answer = await rag_agent.process(perf_query)
            end_time = time.time()
            
            processing_time = end_time - start_time
            print(f"ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            print(f"ë‹µë³€ ê¸¸ì´: {len(answer)}ì")
        except Exception as e:
            print(f"âŒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        
        print("\nğŸ‰ RAG Agent 3 ì‹¤í—˜ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

async def test_url_download_only():
    """URL ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ë§Œ ë‹¨ë… í…ŒìŠ¤íŠ¸"""
    print("ğŸŒ URL ë‹¤ìš´ë¡œë“œ ë‹¨ë… í…ŒìŠ¤íŠ¸...")
    
    from agents.rag_agent3 import download_file_from_url
    
    test_urls = [
        "https://raw.githubusercontent.com/microsoft/vscode/main/README.md",
        "https://www.w3.org/TR/PNG/iso_8859-1.txt",
    ]
    
    for url in test_urls:
        print(f"\nğŸ“¥ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸: {url}")
        try:
            content = await download_file_from_url(url)
            if content:
                print(f"âœ… ì„±ê³µ: {len(content):,} bytes ë‹¤ìš´ë¡œë“œ")
                # í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 200ì)
                try:
                    preview = content.decode('utf-8')[:200]
                    print(f"ë¯¸ë¦¬ë³´ê¸°: {preview}...")
                except:
                    print("ë°”ì´ë„ˆë¦¬ íŒŒì¼ì…ë‹ˆë‹¤.")
            else:
                print("âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ai-serverë¡œ ë³€ê²½
    os.chdir(Path(__file__).parent)
    print(f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    
    # ë©”ë‰´ ì„ íƒ
    print("\nğŸ”¬ RAG Agent 3 ì‹¤í—˜ ë©”ë‰´:")
    print("1. ì „ì²´ ì¢…í•© ì‹¤í—˜")
    print("2. URL ë‹¤ìš´ë¡œë“œë§Œ í…ŒìŠ¤íŠ¸")
    
    choice = input("ì„ íƒí•˜ì„¸ìš” (1-2): ").strip()
    
    if choice == "1":
        asyncio.run(test_rag_agent3())
    elif choice == "2":
        asyncio.run(test_url_download_only())
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ì „ì²´ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        asyncio.run(test_rag_agent3())
