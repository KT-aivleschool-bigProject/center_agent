import os
import asyncio
from typing import List, Dict, Any, Optional
from pathlib import Path
import filetype
import magic
from PIL import Image
import pytesseract
import logging
from datetime import datetime
import pandas as pd
import fitz  # PyMuPDF
from pptx import Presentation

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    DirectoryLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredExcelLoader,
    UnstructuredPowerPointLoader
)
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from typing_extensions import Annotated, TypedDict

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class RAGState(TypedDict):
    """RAG Agentì˜ ìƒíƒœ ì •ì˜"""
    question: str
    retrieved_docs: List[Document]
    answer: str
    error: Optional[str]
    metadata: Dict[str, Any]


class RAGAgent:
    """RAG ê¸°ë°˜ LangGraph Agent"""
    
    def __init__(self):
        self.name = "RAG Agent"
        self.embeddings = None
        self.vectorstore = None
        self.llm = None
        self.retrieval_qa = None
        self.graph = None
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        
        # ë¬¸ì„œ ì €ì¥ ê²½ë¡œ
        self.docs_path = Path("data/docs")
        self.vector_db_path = Path("data/vector_db")
        
        # ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹
        self.supported_extensions = {
            'text': ['.txt', '.md', '.csv'],
            'pdf': ['.pdf'],
            'word': ['.doc', '.docx'],
            'excel': ['.xls', '.xlsx'],
            'powerpoint': ['.ppt', '.pptx'],
            'image': ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'],
            'hwp': ['.hwp']
        }
        
        # ì´ˆê¸°í™”
        self._initialize_components()
        self._setup_graph()
        
        # ë¬¸ì„œëŠ” ì²« ë²ˆì§¸ ìš”ì²­ì‹œ ë¡œë”© (ë¹„ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ ë³„ë„ í˜¸ì¶œ í•„ìš”)
        self._documents_loaded = False
    
    def _initialize_components(self):
        """LangChain êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”"""
        try:
            # OpenAI API í‚¤ í™•ì¸
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                print("  RAG Agent: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return
            
            # LLM ì´ˆê¸°í™”
            self.llm = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0.3,
                api_key=api_key
            )
            
            # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
            self.embeddings = OpenAIEmbeddings(api_key=api_key)
            
            # ë²¡í„° ì €ì¥ì†Œ ë””ë ‰í† ë¦¬ ìƒì„±
            self.vector_db_path.mkdir(parents=True, exist_ok=True)
            
            # ê¸°ì¡´ FAISS ë²¡í„° ì €ì¥ì†Œê°€ ìˆìœ¼ë©´ ë¡œë“œ
            try:
                if (self.vector_db_path / "index.faiss").exists():
                    self.vectorstore = FAISS.load_local(
                        str(self.vector_db_path),
                        self.embeddings,
                        allow_dangerous_deserialization=True
                    )
                    self._setup_retrieval_qa()
                    print("  ê¸°ì¡´ FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                    self._documents_loaded = True
                else:
                    print("  ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¡œë”©í•˜ê² ìŠµë‹ˆë‹¤.")
                    self._documents_loaded = False
            except Exception as e:
                print(f"  FAISS ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self._documents_loaded = False
            
            print(" RAG Agent êµ¬ì„± ìš”ì†Œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f" RAG Agent ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    
    def _setup_graph(self):
        """LangGraph ì›Œí¬í”Œë¡œìš° ì„¤ì •"""
        # ìƒíƒœ ê·¸ë˜í”„ ìƒì„±
        workflow = StateGraph(RAGState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("entry", self._entry_node)
        workflow.add_node("retrieve_docs", self._retrieve_docs_node)
        workflow.add_node("generate_answer", self._generate_answer_node)
        workflow.add_node("output", self._output_node)
        
        # ì—£ì§€ ì„¤ì •
        workflow.set_entry_point("entry")
        workflow.add_edge("entry", "retrieve_docs")
        workflow.add_edge("retrieve_docs", "generate_answer")
        workflow.add_edge("generate_answer", "output")
        workflow.add_edge("output", END)
        
        # ê·¸ë˜í”„ ì»´íŒŒì¼
        self.graph = workflow.compile()
    
    async def _load_documents(self):
        """ë¬¸ì„œ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤ì œ ë¬¸ì„œë“¤ì„ ë¡œë”©"""
        try:
            # ë¬¸ì„œ ë””ë ‰í† ë¦¬ ìƒì„± (ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°)
            self.docs_path.mkdir(parents=True, exist_ok=True)
            
            # ë¬¸ì„œ ë¡œë”© ë° ë²¡í„°í™”
            await self._load_and_vectorize_documents()
            
        except Exception as e:
            print(f" í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ë¡œë”© ì˜¤ë¥˜: {e}")
    
    async def _load_and_vectorize_documents(self):
        """ë¬¸ì„œë¥¼ ë¡œë”©í•˜ê³  ë²¡í„°í™”í•˜ì—¬ ì €ì¥"""
        try:
            if not self.embeddings:
                print("  ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return
            
            documents = []
            
            # í…ìŠ¤íŠ¸ íŒŒì¼ ì§ì ‘ ë¡œë”©
            for txt_file in self.docs_path.glob("*.txt"):
                try:
                    with open(txt_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        doc = Document(
                            page_content=content,
                            metadata={"source": str(txt_file)}
                        )
                        documents.append(doc)
                        print(f" ë¡œë”© ì™„ë£Œ: {txt_file.name}")
                except Exception as e:
                    print(f" {txt_file.name} ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # PDF íŒŒì¼ ë¡œë”©
            for pdf_file in self.docs_path.glob("*.pdf"):
                try:
                    loader = PyPDFLoader(str(pdf_file))
                    docs = loader.load()
                    documents.extend(docs)
                    print(f" PDF ë¡œë”© ì™„ë£Œ: {pdf_file.name}")
                    print(f" PDF ë¡œë”© ì™„ë£Œ: {pdf_file.name}")
                except Exception as e:
                    print(f" PDF {pdf_file.name} ë¡œë”© ì‹¤íŒ¨: {e}")

            # DOCX íŒŒì¼ ë¡œë”©
            for docx_file in self.docs_path.glob("*.docx"):
                try:
                    loader = UnstructuredWordDocumentLoader(str(docx_file))
                    docs = loader.load()
                    documents.extend(docs)
                    print(f" DOCX ë¡œë”© ì™„ë£Œ: {docx_file.name}")
                except Exception as e:
                    print(f" DOCX {docx_file.name} ë¡œë”© ì‹¤íŒ¨: {e}")

            if documents:
                # ë¬¸ì„œ ë¶„í• 
                texts = self.text_splitter.split_documents(documents)
                print(f" {len(documents)}ê°œ ë¬¸ì„œë¥¼ {len(texts)}ê°œ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
                print(f" {len(documents)}ê°œ ë¬¸ì„œë¥¼ {len(texts)}ê°œ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
                
                # ë²¡í„° ì €ì¥ì†Œ ìƒì„±/ì—…ë°ì´íŠ¸
                if self.vectorstore is None:
                    self.vectorstore = FAISS.from_documents(
                        documents=texts,
                        embedding=self.embeddings
                    )
                    # FAISS ì¸ë±ìŠ¤ ì €ì¥
                    self.vectorstore.save_local(str(self.vector_db_path))
                    print(" ìƒˆë¡œìš´ FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
                else:
                    self.vectorstore.add_documents(texts)
                    # FAISS ì¸ë±ìŠ¤ ì €ì¥
                    self.vectorstore.save_local(str(self.vector_db_path))
                    print(" ê¸°ì¡´ FAISS ë²¡í„° ì €ì¥ì†Œì— ë¬¸ì„œë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
                
                # RetrievalQA ì²´ì¸ ì„¤ì •
                self._setup_retrieval_qa()
                
                print(f" ë²¡í„°í™” ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ, {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì²­í¬")
                print(f" ë²¡í„°í™” ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ, {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì²­í¬")
            else:
                print("  ë¡œë”©í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f" ë¬¸ì„œ ë²¡í„°í™” ì˜¤ë¥˜: {e}")
            print(f" ë¬¸ì„œ ë²¡í„°í™” ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
    
    def _setup_retrieval_qa(self):
        """RetrievalQA ì²´ì¸ ì„¤ì •"""
        if not self.vectorstore or not self.llm:
            return
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        prompt_template = """
ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ AIì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì£¼ë¡œ í™œìš©í•˜ë˜, í•„ìš”ì‹œ ì¼ë°˜ì ì¸ ì§€ì‹ë„ í•¨ê»˜ í™œìš©í•˜ì„¸ìš”.
2. ë‹µë³€ì€ ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
3. ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œí•˜ì„¸ìš”.
4. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ë‹µë³€:
"""
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # RetrievalQA ì²´ì¸ ìƒì„±
        self.retrieval_qa = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 4}
            ),
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True
        )
    
    async def _entry_node(self, state: RAGState) -> RAGState:
        """ì§„ì…ì : ì§ˆë¬¸ ì…ë ¥ ì²˜ë¦¬"""
        print(f"ğŸ” RAG Agent: ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘ - {state['question']}")
        
        # ì½”ë“œ ê´€ë ¨ ì§ˆë¬¸ ë¶„ê¸°ì  (í–¥í›„ code_agent í¬ì›Œë”©ìš©)
        question_lower = state["question"].lower()
        code_keywords = ["ì½”ë“œ", "êµ¬í˜„", "ê°œë°œ", "í”„ë¡œê·¸ë˜ë°", "í•¨ìˆ˜", "í´ë˜ìŠ¤", "ë²„ê·¸"]
        
        if any(keyword in question_lower for keyword in code_keywords):
            state["metadata"] = {
                "potential_code_question": True,
                "forward_to_code_agent": False  # í˜„ì¬ëŠ” ë¹„í™œì„±í™”, í–¥í›„ í™•ì¥ì‹œ Trueë¡œ ë³€ê²½
            }
            print("ğŸ’¡ ì½”ë“œ ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ê°ì§€ë¨ (í–¥í›„ code_agent í¬ì›Œë”© ê³ ë ¤)")
        else:
            state["metadata"] = {"potential_code_question": False}
        
        return state
    
    async def _retrieve_docs_node(self, state: RAGState) -> RAGState:
        """ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ"""
        try:
            if not self.vectorstore:
                state["error"] = "ë²¡í„° ì €ì¥ì†Œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¨¼ì € ë¡œë”©í•´ì£¼ì„¸ìš”."
                print(" ë²¡í„° ì €ì¥ì†Œê°€ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œ ë¡œë”©ì„ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.")
                await self._load_documents()  # ë¬¸ì„œ ì¬ë¡œë”© ì‹œë„
                if not self.vectorstore:
                    return state
            
            # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 4}
            )
            
            docs = retriever.get_relevant_documents(state["question"])
            state["retrieved_docs"] = docs
            
            print(f" {len(docs)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤.")
            
            # ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ì„ ê²½ìš° ëŒ€ì²´ ì‘ë‹µ ì¤€ë¹„
            if not docs:
                state["error"] = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                print("  ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            state["error"] = f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            print(f" ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        return state
    
    async def _generate_answer_node(self, state: RAGState) -> RAGState:
        """ë‹µë³€ ìƒì„± ë…¸ë“œ"""
        try:
            if state.get("error"):
                # ì˜¤ë¥˜ê°€ ìˆì§€ë§Œ ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° ëŒ€ì²´ ì‘ë‹µ ìƒì„±
                if "ë²¡í„° ì €ì¥ì†Œ" in state["error"] or "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in state["error"]:
                    state["answer"] = f"""ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë¬¸ì„œ ì €ì¥ì†Œì— ê´€ë ¨ ì •ë³´ê°€ ì—†ì–´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ì§ˆë¬¸: {state["question"]}

ëŒ€ì‹  ì¼ë°˜ì ì¸ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
â€¢ í”„ë¡œì íŠ¸ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
â€¢ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë‹¤ë¥¸ ì „ë¬¸ ì—ì´ì „íŠ¸ê°€ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
â€¢ ì½”ë“œ ê´€ë ¨ ì§ˆë¬¸ì€ 'code' ì—ì´ì „íŠ¸ì—ê²Œ, ë¬¸ì„œ ì‘ì„±ì€ 'document' ì—ì´ì „íŠ¸ì—ê²Œ ë¬¸ì˜í•´ë³´ì„¸ìš”."""
                    state["error"] = None  # ì—ëŸ¬ë¥¼ í´ë¦¬ì–´í•˜ì—¬ ì •ìƒ ì‘ë‹µìœ¼ë¡œ ì²˜ë¦¬
                    print(" ëŒ€ì²´ ì‘ë‹µì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
                    return state
                else:
                    return state
            
            if not self.retrieval_qa:
                state["error"] = "RetrievalQA ì²´ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                return state
            
            # RAG ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„±
            result = self.retrieval_qa({"query": state["question"]})
            state["answer"] = result["result"]
            
            # ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ ì¶”ê°€
            if result.get("source_documents"):
                state["metadata"]["source_count"] = len(result["source_documents"])
                state["answer"] += f"\n\n ì°¸ì¡°í•œ ë¬¸ì„œ: {len(result['source_documents'])}ê°œ"
            
            print(" RAG ë°©ì‹ìœ¼ë¡œ ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            state["error"] = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"
            print(f" ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
        
        return state
    
    async def _output_node(self, state: RAGState) -> RAGState:
        """ì¶œë ¥ ë…¸ë“œ"""
        if state.get("error"):
            state["answer"] = f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {state['error']}"
        
        print(" RAG Agent ì²˜ë¦¬ ì™„ë£Œ")
        return state
    
    async def process(self, message: str) -> str:
        """ë©”ì‹œì§€ ì²˜ë¦¬ (FastAPI í†µí•©ìš©)"""
        try:
            # ì²« ë²ˆì§¸ ìš”ì²­ì‹œ ë¬¸ì„œ ë¡œë”©
            if not self._documents_loaded:
                await self._load_documents()
                self._documents_loaded = True
            
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state = RAGState(
                question=message,
                retrieved_docs=[],
                answer="",
                error=None,
                metadata={}
            )
            
            # ê·¸ë˜í”„ ì‹¤í–‰
            if self.graph:
                result = await self.graph.ainvoke(initial_state)
                return result["answer"]
            else:
                return "RAG Agentê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. OpenAI API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        except Exception as e:
            return f"RAG Agent ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def add_documents(self, file_paths: List[str]) -> str:
        """ìƒˆ ë¬¸ì„œ ì¶”ê°€"""
        try:
            documents = []
            for file_path in file_paths:
                path = Path(file_path)
                if path.suffix == ".txt":
                    loader = TextLoader(str(path))
                elif path.suffix == ".pdf":
                    loader = PyPDFLoader(str(path))
                elif path.suffix == ".docx":
                    loader = UnstructuredWordDocumentLoader(str(path))
                else:
                    continue
                
                docs = loader.load()
                documents.extend(docs)
            
            if documents and self.vectorstore:
                texts = self.text_splitter.split_documents(documents)
                self.vectorstore.add_documents(texts)
                return f"{len(documents)}ê°œ ë¬¸ì„œê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤."
            else:
                return "ì¶”ê°€í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
        
        except Exception as e:
            return f"ë¬¸ì„œ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {str(e)}"

    def _determine_file_type(self, file_path: str) -> str:
        """íŒŒì¼ í˜•ì‹ íŒë³„"""
        path = Path(file_path)
        extension = path.suffix.lower()
        
        for file_type, extensions in self.supported_extensions.items():
            if extension in extensions:
                return file_type
        
        # MIME íƒ€ì…ìœ¼ë¡œ ì¬í™•ì¸
        try:
            mime_type = magic.from_file(file_path, mime=True)
            if mime_type.startswith('text/'):
                return 'text'
            elif mime_type == 'application/pdf':
                return 'pdf'
            elif 'word' in mime_type or 'document' in mime_type:
                return 'word'
            elif 'excel' in mime_type or 'spreadsheet' in mime_type:
                return 'excel'
            elif 'powerpoint' in mime_type or 'presentation' in mime_type:
                return 'powerpoint'
            elif mime_type.startswith('image/'):
                return 'image'
        except:
            pass
        
        return 'unknown'

    def _extract_text_from_image(self, file_path: str) -> str:
        """ì´ë¯¸ì§€ì—ì„œ OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            image = Image.open(file_path)
            text = pytesseract.image_to_string(image, lang='kor+eng')
            return text.strip()
        except Exception as e:
            print(f"ì´ë¯¸ì§€ OCR ì¶”ì¶œ ì‹¤íŒ¨ ({file_path}): {e}")
            return ""

    def _load_single_document(self, file_path: str, project_id: str = None, file_id: str = None) -> List[Document]:
        """ë‹¨ì¼ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        documents = []
        path = Path(file_path)
        file_type = self._determine_file_type(file_path)
        
        metadata = {
            "source": str(path),
            "filename": path.name,
            "file_type": file_type,
        }
        
        if project_id:
            metadata["project_id"] = project_id
        if file_id:
            metadata["file_id"] = file_id
        
        try:
            if file_type == 'text':
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                documents.append(Document(page_content=content, metadata=metadata))
                
            elif file_type == 'pdf':
                loader = PyPDFLoader(file_path)
                docs = loader.load()
                for doc in docs:
                    doc.metadata.update(metadata)
                documents.extend(docs)
                
            elif file_type == 'word':
                loader = UnstructuredWordDocumentLoader(file_path)
                docs = loader.load()
                for doc in docs:
                    doc.metadata.update(metadata)
                documents.extend(docs)
                
            elif file_type == 'excel':
                loader = UnstructuredExcelLoader(file_path)
                docs = loader.load()
                for doc in docs:
                    doc.metadata.update(metadata)
                documents.extend(docs)
                
            elif file_type == 'powerpoint':
                loader = UnstructuredPowerPointLoader(file_path)
                docs = loader.load()
                for doc in docs:
                    doc.metadata.update(metadata)
                documents.extend(docs)
                
            elif file_type == 'image':
                text = self._extract_text_from_image(file_path)
                if text:
                    documents.append(Document(
                        page_content=f"[ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸]\n{text}",
                        metadata=metadata
                    ))
                    
            elif file_type == 'hwp':
                # HWP íŒŒì¼ì€ ë³„ë„ ì²˜ë¦¬ í•„ìš” (í˜„ì¬ëŠ” ìŠ¤í‚µ)
                print(f"HWP íŒŒì¼ì€ í˜„ì¬ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
                
            else:
                print(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path}")
                
        except Exception as e:
            print(f"íŒŒì¼ ë¡œë”© ì‹¤íŒ¨ ({file_path}): {e}")
            
        return documents

    def load_project_documents(self, project_id: str = None) -> List[Document]:
        """ai-server/data/docs ê²½ë¡œì˜ ëª¨ë“  íŒŒì¼ì„ ë¡œë“œ"""
        documents = []
        
        # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
        self.docs_path.mkdir(parents=True, exist_ok=True)
        
        # ì§€ì›í•˜ëŠ” ëª¨ë“  í™•ì¥ì íŒ¨í„´
        all_extensions = []
        for extensions in self.supported_extensions.values():
            all_extensions.extend(extensions)
        
        for extension in all_extensions:
            for file_path in self.docs_path.glob(f"*{extension}"):
                docs = self._load_single_document(str(file_path), project_id)
                documents.extend(docs)
                
        return documents

    async def process_new_document(self, file_path: str, project_id: str = None, file_id: str = None) -> str:
        """ìƒˆë¡œ ì—…ë¡œë“œëœ ë‹¨ì¼ ë¬¸ì„œë¥¼ ì²˜ë¦¬"""
        try:
            logger.info(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {file_path}")
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not Path(file_path).exists():
                return f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"
            
            # ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì´ˆê¸°í™”
            if not self.vectorstore:
                await self._load_documents()
            
            # íŒŒì¼ ë‚´ìš© ì½ê¸°
            content = self.load_document_content(file_path)
            
            if not content.strip():
                return f"íŒŒì¼ì—ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"
            
            # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                "filename": os.path.basename(file_path),
                "project_id": project_id,
                "file_id": file_id,
                "file_path": file_path,
                "upload_time": datetime.now().isoformat(),
                "file_type": self.get_file_extension(file_path),
                "content_length": len(content)
            }
            
            # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
            await self.add_document_to_vector_store(content, metadata)
            
            # RetrievalQA ì¬ì„¤ì •
            self._setup_retrieval_qa()
            
            result_msg = f"ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {metadata['filename']} (í”„ë¡œì íŠ¸: {project_id}, ë‚´ìš© ê¸¸ì´: {len(content)}ì)"
            logger.info(result_msg)
            return result_msg
            
        except Exception as e:
            error_msg = f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            logger.error(error_msg)
            import traceback
            traceback.print_exc()
            return error_msg

    def _check_document_exists(self, file_name: str, project_id: str = None) -> bool:
        """ë¬¸ì„œê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë²¡í„° ìŠ¤í† ì–´ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ê²€ìƒ‰í•´ì•¼ í•¨
        # í˜„ì¬ëŠ” ë‹¨ìˆœíˆ False ë°˜í™˜ (í•­ìƒ ìƒˆ ë¬¸ì„œë¡œ ì²˜ë¦¬)
        return False

    async def _remove_existing_documents(self, file_name: str, project_id: str = None):
        """ê¸°ì¡´ ë¬¸ì„œë¥¼ ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ì œê±°"""
        # FAISSëŠ” ì§ì ‘ì ì¸ ë¬¸ì„œ ì œê±°ê°€ ì–´ë ¤ìš°ë¯€ë¡œ
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ChromaDB ë“± ë‹¤ë¥¸ ë²¡í„° DB ì‚¬ìš© ê¶Œì¥
        pass

    async def search_documents(self, query: str, project_id: str = None, limit: int = 5) -> List[Dict]:
        """ë¬¸ì„œ ê²€ìƒ‰ (í”„ë¡œì íŠ¸ í•„í„°ë§ ì§€ì›)"""
        try:
            if not self.vectorstore:
                return []
            
            # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
            all_docs = self.vectorstore.similarity_search(query, k=limit*2)
            
            # í”„ë¡œì íŠ¸ IDë¡œ í•„í„°ë§ (ì§€ì •ëœ ê²½ìš°)
            if project_id:
                filtered_docs = [
                    doc for doc in all_docs 
                    if doc.metadata.get('project_id') == project_id
                ]
                docs = filtered_docs[:limit]
            else:
                docs = all_docs[:limit]
            
            # ê²°ê³¼ í¬ë§·íŒ…
            results = []
            for doc in docs:
                results.append({
                    "content": doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content,
                    "metadata": doc.metadata,
                    "score": getattr(doc, 'score', 0.0)
                })
            
            logger.info(f"ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ: ì¿¼ë¦¬='{query}', í”„ë¡œì íŠ¸={project_id}, ê²°ê³¼={len(results)}ê°œ")
            return results
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def get_file_extension(self, file_path: str) -> str:
        """íŒŒì¼ í™•ì¥ì ì¶”ì¶œ"""
        return Path(file_path).suffix.lower().lstrip('.')

    def load_document_content(self, file_path: str) -> str:
        """íŒŒì¼ í˜•ì‹ì— ë”°ë¼ ë‚´ìš© ì¶”ì¶œ"""
        file_ext = self.get_file_extension(file_path)
        
        try:
            if file_ext == 'txt':
                return self.load_text_file(file_path)
            elif file_ext == 'pdf':
                return self.load_pdf_file(file_path)
            elif file_ext in ['doc', 'docx']:
                return self.load_word_file(file_path)
            elif file_ext in ['xls', 'xlsx']:
                return self.load_excel_file(file_path)
            elif file_ext in ['ppt', 'pptx']:
                return self.load_powerpoint_file(file_path)
            elif file_ext in ['jpg', 'jpeg', 'png', 'gif', 'bmp', 'webp']:
                return self.extract_text_from_image(file_path)
            elif file_ext == 'hwp':
                logger.warning(f"HWP íŒŒì¼ì€ í˜„ì¬ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
                return ""
            else:
                logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}")
                return ""
        except Exception as e:
            logger.error(f"íŒŒì¼ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {file_path}, ì˜¤ë¥˜: {str(e)}")
            return ""

    def load_text_file(self, file_path: str) -> str:
        """í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()

    def load_pdf_file(self, file_path: str) -> str:
        """PDF íŒŒì¼ ë¡œë“œ (PyMuPDF ì‚¬ìš©)"""
        try:
            doc = fitz.open(file_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            return text
        except Exception as e:
            logger.error(f"PDF íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ëŒ€ì•ˆìœ¼ë¡œ PyPDFLoader ì‚¬ìš©
            try:
                loader = PyPDFLoader(file_path)
                pages = loader.load()
                return "\n".join([page.page_content for page in pages])
            except Exception as e2:
                logger.error(f"PyPDFLoaderë„ ì‹¤íŒ¨: {e2}")
                return ""

    def load_word_file(self, file_path: str) -> str:
        """Word ë¬¸ì„œ ë¡œë“œ"""
        try:
            loader = UnstructuredWordDocumentLoader(file_path)
            docs = loader.load()
            return "\n".join([doc.page_content for doc in docs])
        except Exception as e:
            logger.error(f"Word íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return ""

    def load_excel_file(self, file_path: str) -> str:
        """Excel íŒŒì¼ ë¡œë“œ"""
        try:
            # pandasë¡œ ì½ê¸°
            df = pd.read_excel(file_path, sheet_name=None)  # ëª¨ë“  ì‹œíŠ¸ ì½ê¸°
            content = ""
            for sheet_name, sheet_df in df.items():
                content += f"\n=== ì‹œíŠ¸: {sheet_name} ===\n"
                content += sheet_df.to_string(index=False)
                content += "\n"
            return content
        except Exception as e:
            logger.error(f"Excel íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            try:
                # ëŒ€ì•ˆìœ¼ë¡œ UnstructuredExcelLoader ì‚¬ìš©
                loader = UnstructuredExcelLoader(file_path)
                docs = loader.load()
                return "\n".join([doc.page_content for doc in docs])
            except Exception as e2:
                logger.error(f"UnstructuredExcelLoaderë„ ì‹¤íŒ¨: {e2}")
                return ""

    def load_powerpoint_file(self, file_path: str) -> str:
        """PowerPoint íŒŒì¼ ë¡œë“œ"""
        try:
            prs = Presentation(file_path)
            content = ""
            for i, slide in enumerate(prs.slides):
                content += f"\n=== ìŠ¬ë¼ì´ë“œ {i+1} ===\n"
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        content += shape.text + "\n"
            return content
        except Exception as e:
            logger.error(f"PowerPoint íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            try:
                # ëŒ€ì•ˆìœ¼ë¡œ UnstructuredPowerPointLoader ì‚¬ìš©
                loader = UnstructuredPowerPointLoader(file_path)
                docs = loader.load()
                return "\n".join([doc.page_content for doc in docs])
            except Exception as e2:
                logger.error(f"UnstructuredPowerPointLoaderë„ ì‹¤íŒ¨: {e2}")
                return ""

    def extract_text_from_image(self, file_path: str) -> str:
        """ì´ë¯¸ì§€ì—ì„œ OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            image = Image.open(file_path)
            text = pytesseract.image_to_string(image, lang='kor+eng')
            return f"[ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸]\n{text.strip()}"
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ OCR ì¶”ì¶œ ì‹¤íŒ¨ ({file_path}): {e}")
            return ""

    async def add_document_to_vector_store(self, content: str, metadata: dict):
        """ë¬¸ì„œë¥¼ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€"""
        try:
            if not content.strip():
                logger.warning(f"ë¹ˆ ë¬¸ì„œ ë‚´ìš©: {metadata.get('filename', 'unknown')}")
                return
            
            # ê¸°ì¡´ ë¬¸ì„œ ì¤‘ë³µ ì²´í¬
            filename = metadata.get('filename', '')
            project_id = metadata.get('project_id', '')
            
            existing_doc = self.check_document_exists(filename, project_id)
            if existing_doc:
                logger.info(f"ê¸°ì¡´ ë¬¸ì„œ ì—…ë°ì´íŠ¸: {filename}")
                await self.update_existing_document(content, metadata)
            else:
                logger.info(f"ìƒˆ ë¬¸ì„œ ì¶”ê°€: {filename}")
                await self.add_new_document(content, metadata)
                
        except Exception as e:
            logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            raise

    def check_document_exists(self, filename: str, project_id: str = None) -> bool:
        """ë¬¸ì„œê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        # í˜„ì¬ FAISSëŠ” ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ì´ ì œí•œì ì´ë¯€ë¡œ ë‹¨ìˆœíˆ False ë°˜í™˜
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ChromaDB ë“± ë©”íƒ€ë°ì´í„° í•„í„°ë§ì´ ê°€ëŠ¥í•œ ë²¡í„°DB ì‚¬ìš© ê¶Œì¥
        return False

    async def update_existing_document(self, content: str, metadata: dict):
        """ê¸°ì¡´ ë¬¸ì„œ ì—…ë°ì´íŠ¸"""
        # FAISSëŠ” ì§ì ‘ì ì¸ ì—…ë°ì´íŠ¸ê°€ ì–´ë ¤ìš°ë¯€ë¡œ ìƒˆ ë¬¸ì„œë¡œ ì¶”ê°€
        await self.add_new_document(content, metadata)

    async def add_new_document(self, content: str, metadata: dict):
        """ìƒˆ ë¬¸ì„œ ì¶”ê°€"""
        try:
            # Document ê°ì²´ ìƒì„±
            doc = Document(page_content=content, metadata=metadata)
            
            # ë¬¸ì„œ ë¶„í• 
            texts = self.text_splitter.split_documents([doc])
            
            # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
            if self.vectorstore:
                self.vectorstore.add_documents(texts)
            else:
                # ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                self.vectorstore = FAISS.from_documents(
                    documents=texts,
                    embedding=self.embeddings
                )
            
            # ì €ì¥
            self.vectorstore.save_local(str(self.vector_db_path))
            logger.info(f"ë¬¸ì„œ ë²¡í„°í™” ì™„ë£Œ: {len(texts)}ê°œ ì²­í¬")
            
        except Exception as e:
            logger.error(f"ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise
